{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build a Complete RAG System with 5 Open-Source Tools\n",
    "\n",
    "## Introduction to RAG Systems\n",
    "\n",
    "RAG (Retrieval-Augmented Generation) combines document retrieval with language generation to create intelligent Q&A systems. Instead of relying solely on training data, RAG systems search through your documents to find relevant information, then use that context to generate accurate, source-backed responses.\n",
    "\n",
    "## Environment Setup\n",
    "\n",
    "Install the required libraries for building your RAG pipeline:\n",
    "\n",
    "```bash\n",
    "pip install markitdown[pdf] sentence-transformers langchain-text-splitters chromadb gradio langchain-ollama ollama\n",
    "```\n",
    "\n",
    "These libraries provide:\n",
    "\n",
    "- **[markitdown](https://github.com/microsoft/markitdown)**: Microsoft's document conversion tool that transforms PDFs, Word docs, and other formats into clean markdown\n",
    "- **[sentence-transformers](https://github.com/UKPLab/sentence-transformers)**: Local embedding generation for converting text into searchable vectors\n",
    "- **[langchain-text-splitters](https://github.com/langchain-ai/langchain)**: Intelligent text chunking that preserves semantic meaning\n",
    "- **[chromadb](https://github.com/chroma-core/chroma)**: Self-hosted vector database for storing and querying document embeddings\n",
    "- **[gradio](https://github.com/gradio-app/gradio)**: Web interface builder for creating user-friendly Q&A applications\n",
    "- **[langchain-ollama](https://github.com/langchain-ai/langchain)**: LangChain integration for local LLM inference\n",
    "\n",
    "Install Ollama and download a model:\n",
    "\n",
    "```bash\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "ollama pull llama3.2\n",
    "```\n",
    "\n",
    "Next, create a project directory structure to organize your files:\n",
    "\n",
    "```bash\n",
    "mkdir processed_docs documents\n",
    "```\n",
    "\n",
    "These directories organize your project:\n",
    "\n",
    "- `processed_docs`: Stores converted markdown files\n",
    "- `documents`: Contains original source files (PDFs, Word docs, etc.)\n",
    "\n",
    "Create these directories in your current working path with appropriate read/write permissions.\n",
    "\n",
    "## Dataset Setup: Python Technical Documentation\n",
    "\n",
    "To demonstrate the RAG pipeline, we'll use \"Think Python\" by Allen Downey, a comprehensive programming guide freely available under Creative Commons.\n",
    "\n",
    "We'll download the Python guide and save it in the `documents` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import requests\n",
    "\n",
    "# Get the file path\n",
    "output_folder = \"documents\"\n",
    "filename = \"think_python_guide.pdf\"\n",
    "url = \"https://greenteapress.com/thinkpython/thinkpython.pdf\"\n",
    "file_path = Path(output_folder) / filename\n",
    "\n",
    "\n",
    "def download_file(url: str, file_path: Path):\n",
    "    response = requests.get(url, stream=True, timeout=30)\n",
    "    response.raise_for_status()\n",
    "    file_path.write_bytes(response.content)\n",
    "\n",
    "\n",
    "# Download the file if it doesn't exist\n",
    "if not file_path.exists():\n",
    "    download_file(\n",
    "        url=url,\n",
    "        file_path=file_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's convert this PDF into a format that our RAG system can process and search through.\n",
    "\n",
    "## Document Ingestion with MarkItDown\n",
    "\n",
    "RAG systems need documents in a structured format that AI models can understand and process effectively.\n",
    "\n",
    "MarkItDown solves this challenge by converting any document format into clean markdown while preserving the original structure and meaning.\n",
    "\n",
    "### Converting Your Python Guide\n",
    "\n",
    "Start by converting the Python guide to understand how MarkItDown works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from markitdown import MarkItDown\n",
    "\n",
    "# Initialize the converter\n",
    "md = MarkItDown()\n",
    "\n",
    "# Convert the Python guide to markdown\n",
    "result = md.convert(file_path)\n",
    "python_guide_content = result.text_content\n",
    "\n",
    "# Display the conversion results\n",
    "print(\"First 300 characters:\")\n",
    "print(python_guide_content[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code:\n",
    "\n",
    "- `MarkItDown()` creates a document converter that handles multiple file formats automatically\n",
    "- `convert()` processes the PDF and returns a result object containing the extracted text\n",
    "- `text_content` provides the clean markdown text ready for processing\n",
    "\n",
    "MarkItDown automatically detects the PDF format and extracts clean text while preserving the book's structure, including chapters, sections, and code examples.\n",
    "\n",
    "### Preparing Document for Processing\n",
    "\n",
    "Now that you understand the basic conversion, let's prepare the document content for processing. We'll store the guide's content with source information for later use in chunking and retrieval:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize the converted document\n",
    "processed_document = {\n",
    "    \"source\": file_path,\n",
    "    \"content\": python_guide_content\n",
    "}\n",
    "\n",
    "# Create a list containing our single document for consistency with downstream processing\n",
    "documents = [processed_document]\n",
    "\n",
    "# Document is now ready for chunking and embedding\n",
    "print(f\"Document ready: {len(processed_document['content']):,} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our document successfully converted to markdown, the next step is breaking it into smaller, searchable pieces.\n",
    "\n",
    "## Intelligent Chunking with LangChain\n",
    "\n",
    "AI models can't process entire documents due to limited context windows. Chunking breaks documents into smaller, searchable pieces while preserving semantic meaning.\n",
    "\n",
    "## Understanding Text Chunking with a Simple Example\n",
    "\n",
    "Let's see how text chunking works with a simple document:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Create a simple example that will be split\n",
    "sample_text = \"\"\"\n",
    "Machine learning transforms data processing. It enables pattern recognition without explicit programming.\n",
    "\n",
    "Deep learning uses neural networks with multiple layers. These networks discover complex patterns automatically.\n",
    "\n",
    "Natural language processing combines ML with linguistics. It helps computers understand human language effectively.\n",
    "\"\"\"\n",
    "\n",
    "# Apply chunking with smaller size to demonstrate splitting\n",
    "demo_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,  # Small size to force splitting\n",
    "    chunk_overlap=30,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],  # Split hierarchy\n",
    ")\n",
    "\n",
    "sample_chunks = demo_splitter.split_text(sample_text.strip())\n",
    "\n",
    "print(f\"Original: {len(sample_text.strip())} chars → {len(sample_chunks)} chunks\")\n",
    "\n",
    "# Show chunks\n",
    "for i, chunk in enumerate(sample_chunks):\n",
    "    print(f\"Chunk {i+1}: {chunk}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the text splitter:\n",
    "\n",
    "- Split the 336-character text into 3 chunks, each under the 150-character limit\n",
    "- Applied 30-character overlap between adjacent chunks\n",
    "- Separators prioritize semantic boundaries: paragraphs (`\\n\\n`) → sentences (`. `) → words (` `) → characters\n",
    "\n",
    "## Processing Multiple Documents at Scale\n",
    "\n",
    "Now let's a text splitter with larger chunks and apply it to all our converted documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the text splitter with Q&A-optimized settings\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=600,         # Optimal chunk size for Q&A scenarios\n",
    "    chunk_overlap=120,      # 20% overlap to preserve context\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Split hierarchy\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use the text splitter to process all our documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(doc, text_splitter):\n",
    "    \"\"\"Process a single document into chunks.\"\"\"\n",
    "    doc_chunks = text_splitter.split_text(doc[\"content\"])\n",
    "    return [{\"content\": chunk, \"source\": doc[\"source\"]} for chunk in doc_chunks]\n",
    "\n",
    "\n",
    "# Process all documents and create chunks\n",
    "all_chunks = []\n",
    "for doc in documents:\n",
    "    doc_chunks = process_document(doc, text_splitter)\n",
    "    all_chunks.extend(doc_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examine how the chunking process distributed content across our documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "source_counts = Counter(chunk[\"source\"] for chunk in all_chunks)\n",
    "chunk_lengths = [len(chunk[\"content\"]) for chunk in all_chunks]\n",
    "\n",
    "print(f\"Total chunks created: {len(all_chunks)}\")\n",
    "print(f\"Chunk length: {min(chunk_lengths)}-{max(chunk_lengths)} characters\")\n",
    "print(f\"Source document: {Path(documents[0]['source']).name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our text chunks are ready. Next, we'll transform them into a format that enables intelligent similarity search.\n",
    "\n",
    "## Creating Searchable Embeddings with SentenceTransformers\n",
    "\n",
    "RAG systems need to understand text meaning, not just match keywords. SentenceTransformers converts your text into numerical vectors that capture semantic relationships, allowing the system to find truly relevant information even when exact words don't match.\n",
    "\n",
    "### Generate Embeddings\n",
    "\n",
    "Let's generate embeddings for our text chunks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Load Q&A-optimized embedding model (downloads automatically on first use)\n",
    "model = SentenceTransformer(\"multi-qa-mpnet-base-dot-v1\")\n",
    "\n",
    "# Extract documents and create embeddings\n",
    "documents = [chunk[\"content\"] for chunk in all_chunks]\n",
    "embeddings = model.encode(documents)\n",
    "\n",
    "print(\"Embedding generation results:\")\n",
    "print(f\"  - Embeddings shape: {embeddings.shape}\")\n",
    "print(f\"  - Vector dimensions: {embeddings.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code:\n",
    "\n",
    "- `SentenceTransformer()` loads the Q&A-optimized model that converts text to 768-dimensional vectors\n",
    "- `multi-qa-mpnet-base-dot-v1` is specifically trained on 215M question-answer pairs for superior Q&A performance\n",
    "- `model.encode()` transforms all text chunks into numerical embeddings in a single batch operation\n",
    "\n",
    "The output shows 1007 chunks converted to 768-dimensional vectors.\n",
    "\n",
    "### Test Semantic Similarity\n",
    "\n",
    "Let's test semantic similarity by querying for Python programming concepts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test how one query finds relevant Python programming content\n",
    "from sentence_transformers import util\n",
    "\n",
    "query = \"How do you define functions in Python?\"\n",
    "document_chunks = [\n",
    "    \"Variables store data values that can be used later in your program.\",\n",
    "    \"A function is a block of code that performs a specific task when called.\",\n",
    "    \"Loops allow you to repeat code multiple times efficiently.\",\n",
    "    \"Functions can accept parameters and return values to the calling code.\"\n",
    "]\n",
    "\n",
    "# Encode query and documents\n",
    "query_embedding = model.encode(query)\n",
    "doc_embeddings = model.encode(document_chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll calculate similarity scores and rank the results. The `util.cos_sim()` function computes cosine similarity between vectors, returning values from 0 (no similarity) to 1 (identical meaning):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarities using SentenceTransformers util\n",
    "similarities = util.cos_sim(query_embedding, doc_embeddings)[0]\n",
    "\n",
    "# Create ranked results\n",
    "ranked_results = sorted(\n",
    "    zip(document_chunks, similarities, strict=False), \n",
    "    key=lambda x: x[1], \n",
    "    reverse=True\n",
    ")\n",
    "\n",
    "print(f\"Query: '{query}'\")\n",
    "print(\"Document chunks ranked by relevance:\")\n",
    "for i, (chunk, score) in enumerate(ranked_results, 1):\n",
    "    print(f\"{i}. ({score:.3f}): '{chunk}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity scores demonstrate semantic understanding: function-related chunks achieve high scores (0.7+) while unrelated programming concepts score much lower (0.2-).\n",
    "\n",
    "## Building Your Knowledge Database with ChromaDB\n",
    "\n",
    "These embeddings demonstrate semantic search capability, but memory storage has scalability limitations. Large vector collections quickly exhaust system resources.\n",
    "\n",
    "Vector databases provide essential production capabilities:\n",
    "\n",
    "- **Persistent storage**: Data survives system restarts and crashes\n",
    "- **Optimized indexing**: Fast similarity search using HNSW algorithms\n",
    "- **Memory efficiency**: Handles millions of vectors without RAM exhaustion\n",
    "- **Concurrent access**: Multiple users query simultaneously\n",
    "- **Metadata filtering**: Search by document properties and attributes\n",
    "\n",
    "ChromaDB delivers these features with a Python-native API that integrates seamlessly into your existing data pipeline.\n",
    "\n",
    "### Initialize Vector Database\n",
    "\n",
    "First, we'll set up the ChromaDB client and create a collection to store our document vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# Create persistent client for data storage\n",
    "client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "# Create collection for business documents (or get existing)\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"python_guide\",\n",
    "    metadata={\"description\": \"Python programming guide\"}\n",
    ")\n",
    "\n",
    "print(f\"Created collection: {collection.name}\")\n",
    "print(f\"Collection ID: {collection.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code:\n",
    "\n",
    "- `PersistentClient(path=\"./chroma_db\")` creates a local vector database that persists data to disk\n",
    "- `get_or_create_collection()` creates a new collection or returns an existing one with the same name\n",
    "\n",
    "### Store Documents with Metadata\n",
    "\n",
    "Now we'll store our document chunks with basic metadata in ChromaDB with the `add()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare metadata and add documents to collection\n",
    "metadatas = [{\"document\": Path(chunk[\"source\"]).name} for chunk in all_chunks]\n",
    "\n",
    "collection.add(\n",
    "    documents=documents,\n",
    "    embeddings=embeddings.tolist(), # Convert numpy array to list\n",
    "    metadatas=metadatas, # Metadata for each document\n",
    "    ids=[f\"doc_{i}\" for i in range(len(documents))], # Unique identifiers for each document\n",
    ")\n",
    "\n",
    "print(f\"Collection count: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The database now contains 1007 searchable document chunks with their vector embeddings. ChromaDB persists this data to disk, enabling instant queries without reprocessing documents on restart.\n",
    "\n",
    "### Query the Knowledge Base\n",
    "\n",
    "Let's search the vector database using natural language questions and retrieve relevant document chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_query_results(question, query_embedding, documents, metadatas):\n",
    "    \"\"\"Format and print the search results with similarity scores\"\"\"\n",
    "    from sentence_transformers import util\n",
    "\n",
    "    print(f\"Question: {question}\\n\")\n",
    "\n",
    "    for i, doc in enumerate(documents):\n",
    "        # Calculate accurate similarity using sentence-transformers util\n",
    "        doc_embedding = model.encode([doc])\n",
    "        similarity = util.cos_sim(query_embedding, doc_embedding)[0][0].item()\n",
    "        source = metadatas[i].get(\"document\", \"Unknown\")\n",
    "\n",
    "        print(f\"Result {i+1} (similarity: {similarity:.3f}):\")\n",
    "        print(f\"Document: {source}\")\n",
    "        print(f\"Content: {doc[:300]}...\")\n",
    "        print()\n",
    "\n",
    "\n",
    "def query_knowledge_base(question, n_results=2):\n",
    "    \"\"\"Query the knowledge base with natural language\"\"\"\n",
    "    # Encode the query using our SentenceTransformer model\n",
    "    query_embedding = model.encode([question])\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding.tolist(),\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"],\n",
    "    )\n",
    "\n",
    "    # Extract results and format them\n",
    "    documents = results[\"documents\"][0]\n",
    "    metadatas = results[\"metadatas\"][0]\n",
    "\n",
    "    format_query_results(question, query_embedding, documents, metadatas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code:\n",
    "\n",
    "- `collection.query()` performs vector similarity search using the question text as input\n",
    "- `query_texts` accepts a list of natural language questions for batch processing\n",
    "- `n_results` limits the number of most similar documents returned\n",
    "- `include` specifies which data to return: document text, metadata, and similarity distances\n",
    "\n",
    "Let's test the query function with a question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_knowledge_base(\"How do if-else statements work in Python?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The search finds relevant content with strong similarity scores (0.636 and 0.605).\n",
    "\n",
    "## Enhanced Answer Generation with Open-Source LLMs\n",
    "\n",
    "Vector similarity search retrieves related content, but the results may be scattered across multiple chunks without forming a complete answer.\n",
    "\n",
    "LLMs solve this by weaving retrieved context into unified responses that directly address user questions.\n",
    "\n",
    "In this section, we'll integrate Ollama's local LLMs with our vector search to generate coherent answers from retrieved chunks.\n",
    "\n",
    "### Answer Generation Implementation\n",
    "\n",
    "First, set up the components for LLM-powered answer generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_ollama import OllamaLLM\n",
    "\n",
    "# Initialize the local LLM\n",
    "llm = OllamaLLM(model=\"llama3.2:latest\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, create a focused prompt template for technical documentation queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"You are a Python programming expert. Based on the provided documentation, answer the question clearly and accurately.\n",
    "\n",
    "Documentation:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer (be specific about syntax, keywords, and provide examples when helpful):\"\"\"\n",
    ")\n",
    "\n",
    "# Create the processing chain\n",
    "chain = prompt_template | llm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to retrieve relevant context given a question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_context(question, n_results=5):\n",
    "    \"\"\"Retrieve relevant context using embeddings\"\"\"\n",
    "    query_embedding = model.encode([question])\n",
    "    results = collection.query(\n",
    "        query_embeddings=query_embedding.tolist(),\n",
    "        n_results=n_results,\n",
    "        include=[\"documents\", \"metadatas\", \"distances\"],\n",
    "    )\n",
    "\n",
    "    documents = results[\"documents\"][0]\n",
    "    context = \"\\n\\n---SECTION---\\n\\n\".join(documents)\n",
    "    return context, documents\n",
    "\n",
    "\n",
    "def get_llm_answer(question, context):\n",
    "    \"\"\"Generate answer using retrieved context\"\"\"\n",
    "    answer = chain.invoke(\n",
    "        {\n",
    "            \"context\": context[:2000],\n",
    "            \"question\": question,\n",
    "        }\n",
    "    )\n",
    "    return answer\n",
    "\n",
    "\n",
    "def format_response(question, answer, source_chunks):\n",
    "    \"\"\"Format the final response with sources\"\"\"\n",
    "    response = f\"**Question:** {question}\\n\\n\"\n",
    "    response += f\"**Answer:** {answer}\\n\\n\"\n",
    "    response += \"**Sources:**\\n\"\n",
    "\n",
    "    for i, chunk in enumerate(source_chunks[:3], 1):\n",
    "        preview = chunk[:100].replace(\"\\n\", \" \") + \"...\"\n",
    "        response += f\"{i}. {preview}\\n\"\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "def enhanced_query_with_llm(question, n_results=5):\n",
    "    \"\"\"Query function combining retrieval with LLM generation\"\"\"\n",
    "    context, documents = retrieve_context(question, n_results)\n",
    "    answer = get_llm_answer(question, context)\n",
    "    return format_response(question, answer, documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Enhanced Answer Generation\n",
    "\n",
    "Let's test the enhanced system with our challenging question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the enhanced query system\n",
    "enhanced_response = enhanced_query_with_llm(\"How do if-else statements work in Python?\")\n",
    "print(enhanced_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the LLM organizes multiple chunks into logical sections with syntax examples and step-by-step explanations. This transformation turns raw retrieval into actionable programming guidance.\n",
    "\n",
    "### Streaming Interface Implementation\n",
    "\n",
    "Users now expect the real-time streaming experience from ChatGPT and Claude. Static responses that appear all at once feel outdated and create an impression of poor performance.\n",
    "\n",
    "Token-by-token streaming bridges this gap by creating the familiar typing effect that signals active processing.\n",
    "\n",
    "To implement a streaming interface, we'll use the `chain.stream()` method to generate tokens one at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stream_llm_answer(question, context):\n",
    "    \"\"\"Stream LLM answer generation token by token\"\"\"\n",
    "    for chunk in chain.stream({\n",
    "        \"context\": context[:2000],\n",
    "        \"question\": question,\n",
    "    }):\n",
    "        yield getattr(chunk, \"content\", str(chunk))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how streaming works by combining our modular functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Test the streaming functionality\n",
    "question = \"What are Python loops?\"\n",
    "context, documents = retrieve_context(question, n_results=3)\n",
    "\n",
    "print(\"Question:\", question)\n",
    "print(\"Answer: \", end=\"\", flush=True)\n",
    "\n",
    "# Stream the answer token by token\n",
    "for token in stream_llm_answer(question, context):\n",
    "    print(token, end=\"\", flush=True)\n",
    "    time.sleep(0.05)  # Simulate real-time typing effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This creates the familiar ChatGPT-style typing animation where tokens appear progressively.\n",
    "\n",
    "## Building a Simple Application with Gradio\n",
    "\n",
    "Now that we have a complete RAG system with enhanced answer generation, let's make it accessible through a web interface.\n",
    "\n",
    "Your RAG system needs an intuitive interface that non-technical users can access easily. Gradio provides this solution with:\n",
    "\n",
    "- **Zero web development required**: Create interfaces directly from Python functions\n",
    "- **Automatic UI generation**: Input fields and buttons generated automatically\n",
    "- **Instant deployment**: Launch web apps with a single line of code\n",
    "\n",
    "### Interface Function\n",
    "\n",
    "Let's create the complete Gradio interface that combines the functions we've built into a streaming RAG system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "def rag_interface(question):\n",
    "    \"\"\"Gradio interface reusing existing format_response function\"\"\"\n",
    "    if not question.strip():\n",
    "        yield \"Please enter a question.\"\n",
    "        return\n",
    "\n",
    "    # Use modular retrieval and streaming\n",
    "    context, documents = retrieve_context(question, n_results=5)\n",
    "\n",
    "    response_start = f\"**Question:** {question}\\n\\n**Answer:** \"\n",
    "    answer = \"\"\n",
    "\n",
    "    # Stream the answer progressively\n",
    "    for token in stream_llm_answer(question, context):\n",
    "        answer += token\n",
    "        yield response_start + answer\n",
    "\n",
    "    # Use existing formatting function for final response\n",
    "    yield format_response(question, answer, documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application Setup and Launch\n",
    "\n",
    "Now, we'll configure the Gradio web interface with sample questions and launch the application for user access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Gradio interface with streaming support\n",
    "demo = gr.Interface(\n",
    "    fn=rag_interface,\n",
    "    inputs=gr.Textbox(\n",
    "        label=\"Ask a question about Python programming\",\n",
    "        placeholder=\"How do if-else statements work in Python?\",\n",
    "        lines=2,\n",
    "    ),\n",
    "    outputs=gr.Markdown(label=\"Answer\"),\n",
    "    title=\"Intelligent Document Q&A System\",\n",
    "    description=\"Ask questions about Python programming concepts and get instant answers with source citations.\",\n",
    "    examples=[\n",
    "        \"How do if-else statements work in Python?\",\n",
    "        \"What are the different types of loops in Python?\",\n",
    "        \"How do you handle errors in Python?\",\n",
    "    ],\n",
    "    allow_flagging=\"never\",\n",
    ")\n",
    "\n",
    "# Launch the interface with queue enabled for streaming\n",
    "if __name__ == \"__main__\":\n",
    "    demo.queue().launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3",
   "path": "/Users/khuyentran/codecut_content/codecut_articles/.venv/share/jupyter/kernels/python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
